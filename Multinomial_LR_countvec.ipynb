{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression\n",
    "\n",
    "** Names: ** Barbara, Eva & Joyce\n",
    "<br><br>\n",
    "In this notebook we will implement multinomial logistic regression using countvectorizer to represent our data (rather than creating matrices from scratch). Then we will look at the feature importance of this model.\n",
    "\n",
    "\n",
    "### Index\n",
    "1. ** Logistic Regression using `Count Vectorizer` **\n",
    "    \n",
    "    \n",
    "2. ** Feature importance **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using CountVectorizer to represent data and apply LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  truth-value                                               text  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "\n",
       "                                topic            name                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "\n",
       "      state    politics  count1  count2  count3  count4  count5  \\\n",
       "0     Texas  republican     0.0     1.0     0.0     0.0     0.0   \n",
       "1  Virginia    democrat     0.0     0.0     1.0     1.0     0.0   \n",
       "2  Illinois    democrat    70.0    71.0   160.0   163.0     9.0   \n",
       "\n",
       "           context  \n",
       "0         a mailer  \n",
       "1  a floor speech.  \n",
       "2           Denver  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe for the train data\n",
    "df_liar = pd.read_csv(\"train.tsv\", encoding=\"utf8\", sep=\"\\t\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\"])\n",
    "\n",
    "df_liar.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements without labels: 0\n"
     ]
    }
   ],
   "source": [
    "# classification formula to take return the corresponding number to the validity label\n",
    "validity_labels = {\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "nolabel = 0 \n",
    "def classify_validity(text):\n",
    "    if text not in validity_labels.keys():          #in the case that the label is different\n",
    "        return -1\n",
    "        nolabel += 1\n",
    "    else:\n",
    "        return validity_labels[text]\n",
    "    \n",
    "df_liar[\"truth-score\"] = df_liar[\"truth-value\"].apply(classify_validity)  #add column with truth-score which contains a validity number\n",
    "print(\"Number of statements without labels:\", nolabel)                    #check how many statement do not one of the 6 validity labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since all staments have a label, we do not have to filter out specific labels.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply Count Vectorizer in order to represent the data of the train dataset which we can later use for multinomial logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit(df_liar.text)          #our X matrix is the text from the statements \n",
    "X_train = count_vect.transform(df_liar.text)    \n",
    "\n",
    "y_train = df_liar[\"truth-score\"].values         #our y vector is the list of all the truth labels     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a dataframe of the test dataset in order to test our logistic regression model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11972.json</td>\n",
       "      <td>true</td>\n",
       "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>42</td>\n",
       "      <td>23</td>\n",
       "      <td>18</td>\n",
       "      <td>Radio interview</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11685.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Wisconsin is on pace to double the number of l...</td>\n",
       "      <td>jobs</td>\n",
       "      <td>katrina-shankland</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>democrat</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a news conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11096.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says John McCain has done nothing to help the ...</td>\n",
       "      <td>military,veterans,voting-record</td>\n",
       "      <td>donald-trump</td>\n",
       "      <td>President-Elect</td>\n",
       "      <td>New York</td>\n",
       "      <td>republican</td>\n",
       "      <td>63</td>\n",
       "      <td>114</td>\n",
       "      <td>51</td>\n",
       "      <td>37</td>\n",
       "      <td>61</td>\n",
       "      <td>comments on ABC's This Week.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id truth-value                                               text  \\\n",
       "0  11972.json        true  Building a wall on the U.S.-Mexico border will...   \n",
       "1  11685.json       false  Wisconsin is on pace to double the number of l...   \n",
       "2  11096.json       false  Says John McCain has done nothing to help the ...   \n",
       "\n",
       "                             topic               name                   job  \\\n",
       "0                      immigration         rick-perry              Governor   \n",
       "1                             jobs  katrina-shankland  State representative   \n",
       "2  military,veterans,voting-record       donald-trump       President-Elect   \n",
       "\n",
       "       state    politics  count1  count2  count3  count4  count5  \\\n",
       "0      Texas  republican      30      30      42      23      18   \n",
       "1  Wisconsin    democrat       2       1       0       0       0   \n",
       "2   New York  republican      63     114      51      37      61   \n",
       "\n",
       "                        context  \n",
       "0               Radio interview  \n",
       "1             a news conference  \n",
       "2  comments on ABC's This Week.  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe for the test data\n",
    "df_liar_test = pd.read_csv(\"test.tsv\", encoding=\"utf8\", sep=\"\\t\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\"])\n",
    "\n",
    "df_liar_test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_liar_test[\"truth-score\"] = df_liar_test[\"truth-value\"].apply(classify_validity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = count_vect.transform(df_liar_test.text)        # X matrix is again text from statements\n",
    "y_test = df_liar_test[\"truth-score\"].values             # y vector is list of all the truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.244672454617206\n",
      "310\n"
     ]
    }
   ],
   "source": [
    " y_hat_test = lr.predict(X_test)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "> The accuracy of this multinomial model is unfortunately very low. However, we expected the accuracy to be low, as the accuracy of the binomial model was only around 60%. As there are 6 different labels for validity in this case with some labels between false and true the difference between the statements is smaller than in the case of only true and false and thus probably more difficult to distinguish. We saw from the binomial model that it was already difficult to predict the label of the false and true statements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Importance \n",
    "Here we create dictionaries for all the features, which are all the different words of the train dataset, with their corresponding weight in this logistic regression model. All the words have a different weight for the six different labels. We therefor have six dictionaries with all the words and their importance weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.19671048  0.03022712 -0.04513979 ... -0.04800402 -0.02337375\n",
      "  -0.07402812]\n",
      " [-0.13880998 -0.06395065  0.27090762 ... -0.01712352  0.30305471\n",
      "  -0.21690808]\n",
      " [-0.18554702  0.2231832  -0.05050444 ... -0.13676568 -0.05250957\n",
      "  -0.04384467]\n",
      " [ 0.52984746  0.2695472  -0.08310035 ...  0.33815024 -0.17182523\n",
      "  -0.12282034]\n",
      " [-0.27025362 -0.09350222 -0.05699162 ... -0.11081243 -0.04979296\n",
      "  -0.21307652]\n",
      " [ 0.26147363 -0.36550465 -0.03517143 ... -0.02544459 -0.0055532\n",
      "   0.67067773]]\n",
      "(6, 12196)\n"
     ]
    }
   ],
   "source": [
    "print(lr.coef_)\n",
    "print(lr.coef_.shape)\n",
    "# This coefficient matrix has 6 rows as there are 6 different labels. \n",
    "#For every label each word has a different weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficient dictionaries for the different labels\n",
    "\n",
    "#label false \n",
    "coef_dict_false = dict()                                    \n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict_false[key] = lr.coef_[0][n] \n",
    "    \n",
    "#label barely-true\n",
    "coef_dict_barelytrue = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict_barelytrue[key] = lr.coef_[1][n] \n",
    "\n",
    "#label half-true\n",
    "coef_dict_halftrue = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict_halftrue[key] = lr.coef_[2][n] \n",
    "    \n",
    "#label mostly-true\n",
    "coef_dict_mostlytrue = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict_mostlytrue[key] = lr.coef_[3][n] \n",
    "    \n",
    "#label true\n",
    "coef_dict_true = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict_true[key] = lr.coef_[4][n] \n",
    "    \n",
    "#label pants-fire \n",
    "coef_dict_pantsfire = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict_pantsfire[key] = lr.coef_[5][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordering the different coefficient dictionaries\n",
    "\n",
    "ordered_coefs_false = [(k, coef_dict_false[k]) for k in sorted(coef_dict_false, key=coef_dict_false.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_barelytrue = [(k, coef_dict_barelytrue[k]) for k in sorted(coef_dict_barelytrue, key=coef_dict_barelytrue.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_halftrue = [(k, coef_dict_halftrue[k]) for k in sorted(coef_dict_halftrue, key=coef_dict_halftrue.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_mostlytrue = [(k, coef_dict_mostlytrue[k]) for k in sorted(coef_dict_mostlytrue, key=coef_dict_mostlytrue.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_true = [(k, coef_dict_true[k]) for k in sorted(coef_dict_true, key=coef_dict_true.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_pantsfire = [(k, coef_dict_pantsfire[k]) for k in sorted(coef_dict_pantsfire, key=coef_dict_pantsfire.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('colleges', 1.571115551590921),\n",
       " ('crystal', 1.5501536971495709),\n",
       " ('ranches', 1.3072655297420652),\n",
       " ('pray', 1.289434031495409),\n",
       " ('turkeys', 1.2784723202137627),\n",
       " ('er', 1.2682674094090063),\n",
       " ('psychiatric', 1.2576381806723511),\n",
       " ('directing', 1.2305376319688748),\n",
       " ('to1', 1.2288779585651446),\n",
       " ('sam', 1.221000814601286)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_false[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('recount', 1.4426736703720562),\n",
       " ('dold', 1.2766246724560313),\n",
       " ('pastor', 1.2701726512090712),\n",
       " ('doc', 1.2375502252426396),\n",
       " ('initiated', 1.234166400553806),\n",
       " ('crucifying', 1.2327905840487676),\n",
       " ('declined', 1.2185644260883728),\n",
       " ('fivefold', 1.183293722693532),\n",
       " ('basra', 1.1813164923510588),\n",
       " ('violations', 1.1751700467412065)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_barelytrue[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('megachurch', 1.7285743052031848),\n",
       " ('push', 1.5753411395139432),\n",
       " ('begging', 1.531972208134943),\n",
       " ('experiment', 1.487670453158909),\n",
       " ('280', 1.34768206982658),\n",
       " ('carved', 1.2822690735233273),\n",
       " ('524', 1.2430842418501653),\n",
       " ('abu', 1.2391937734386518),\n",
       " ('palestine', 1.2224019220778954),\n",
       " ('morris', 1.1783782962220712)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_halftrue[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('swastika', 1.8035766417854084),\n",
       " ('kittens', 1.7975403734980244),\n",
       " ('usual', 1.59153021155955),\n",
       " ('treasury', 1.4795967576923121),\n",
       " ('windfall', 1.2986497430970094),\n",
       " ('indianas', 1.268607903507159),\n",
       " ('assaulted', 1.2567707974363818),\n",
       " ('independent', 1.2565552123023043),\n",
       " ('wouldnt', 1.245281637956506),\n",
       " ('1928', 1.229870338883784)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_mostlytrue[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('science', 1.6067734871329367),\n",
       " ('611', 1.4853807170205966),\n",
       " ('1968', 1.4386097244832259),\n",
       " ('minutemen', 1.389352344114847),\n",
       " ('toddlers', 1.3195292753787937),\n",
       " ('istheone', 1.2913603536043332),\n",
       " ('eggs', 1.2667774710282504),\n",
       " ('ticketing', 1.221552578683125),\n",
       " ('internment', 1.1896678838816612),\n",
       " ('peaceful', 1.1562809128213787)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_true[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spoke', 1.9713198139783084),\n",
       " ('husted', 1.5713416375593625),\n",
       " ('handful', 1.5511399379652162),\n",
       " ('jurors', 1.4219543689550154),\n",
       " ('skips', 1.401469745657929),\n",
       " ('battery', 1.3530141100001047),\n",
       " ('mode', 1.351160825791204),\n",
       " ('frated', 1.3398404942254272),\n",
       " ('civil', 1.3290797741668097),\n",
       " ('children', 1.2737802434821182)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_pantsfire[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    ">Here we can see which words, for each label, were important in determining its label. Interestingly, the words with the highest weigt for \"true\", are different from the words with the highest weight in binomial logistic regression. Furthermore, the words with the lowest weight in the binomial logistic regression, which were the words contributed most to labeling the statement as false, did not correspond with the highest weights of the \"false\" label. As there are multiple labels in this case, the whole model is ofcourse different, however we would have expected some simmilarities. Furthermore some of the words with the highest weights seem very strange to actually be of importance in determining the label of the statement such as \"611\" and \"to1\". However, as our model has very low accuracy, we cannot really extract useful information from these coefficient matrices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
