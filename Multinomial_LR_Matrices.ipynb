{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Multinomial Linear Regression\n",
    "**Names:** Eva, Barbara & Joyce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we started implementing multinomial Linear Regression by creating matrices with vocabulary counts from scratch. However, in the end we did not use this code to compare our multinomial linear regression models, as it was easier to implement linear regression in a similar way by making use of Count Vectorizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a list \"allwords\" where we add all the words that occur in the statements of the train dataset. We make sure that the words are lemmatized and all set to lowercase, so that they are case insensitive. From this list we can make a vocabulary, a list containing all the distinct words that occur in the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in trainset: 184014\n",
      "Number of distinct words in trainset: 11916\n",
      "Number of statements in train dataset 10240\n",
      "['say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand', 'when', 'did', 'the', 'decline', 'of', 'coal', 'start', 'it', 'started', 'when', 'natural', 'gas', 'took', 'off']\n",
      "['say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand', 'when', 'did', 'decline', 'of', 'coal', 'start', 'it', 'started', 'natural', 'gas', 'took', 'off', 'that', 'to']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "import string\n",
    "\n",
    "countlines = 0 \n",
    "\n",
    "allwords = []\n",
    "with open(\"train.tsv\", encoding=\"utf8\") as tsvfile:    #open training set\n",
    "    lines = csv.reader(tsvfile, delimiter=\"\\t\")        #convert file to lines\n",
    "    for line in lines:\n",
    "        statement = line[2]                     #get statement from each line \n",
    "        lostrings = statement.split(\" \")        #convert string to list of strings\n",
    "        new_lostrings = []\n",
    "        countlines += 1                         #count number of lines so we know the number of statements\n",
    "        for word in lostrings:\n",
    "            word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "            new_lostrings.append(word)\n",
    "        allwords.extend(new_lostrings)\n",
    "\n",
    "vocab = []                          #initialize a list for all the distint words in the trainset\n",
    "for word in allwords:\n",
    "    if word in vocab:               #do not add word if word is already in vocabulary \n",
    "        continue \n",
    "    else:\n",
    "        vocab.append(word)\n",
    "\n",
    "print(\"Number of words in trainset:\", len(allwords))\n",
    "print(\"Number of distinct words in trainset:\", len(vocab))\n",
    "print(\"Number of statements in train dataset\",countlines)\n",
    "print(allwords[:25])\n",
    "print(vocab[:25])    #here we can see that the vocabulary only contains distinct words \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will make a list with all the statements of the train dataset. Here we again lemmatize all the words, remove the punctuation and set the words to lowercase. Furthermore we will make a list containing all the corresponding validity labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 statements: [['say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand'], ['when', 'did', 'the', 'decline', 'of', 'coal', 'start', 'it', 'started', 'when', 'natural', 'gas', 'took', 'off', 'that', 'started', 'to', 'begin', 'in', 'president', 'george', 'w', 'bush', 'administration'], ['hillary', 'clinton', 'agrees', 'with', 'john', 'mccain', 'by', 'voting', 'to', 'give', 'george', 'bush', 'the', 'benefit', 'of', 'the', 'doubt', 'on', 'iran']]\n",
      "First 3 labels: ['false', 'half-true', 'mostly-true']\n"
     ]
    }
   ],
   "source": [
    "statements = []\n",
    "labels = []\n",
    "\n",
    "with open(\"train.tsv\", encoding=\"utf8\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        label = line[1]\n",
    "        aline = line[2]\n",
    "        bline = aline.split(\" \")\n",
    "        cline = []\n",
    "        for word in bline:\n",
    "            word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "            cline.append(word)\n",
    "        labels.append(label)\n",
    "        statements.append(cline)\n",
    "\n",
    "print(\"First 3 statements:\", statements[:3])\n",
    "print(\"First 3 labels:\", labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create an empty matrix in which we will later on represent all the statements. Each row of the matrix corresponds to a statement and the columns to the words of the vocabulary. By creating such a matrix we can demonstrate for each statement if and how often words of the vocabulary occur in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "rows = countlines          #corresponds to the number of statements of the train set\n",
    "columns = len(vocab)       #corresponds to the number of distinct words occuring in the train set \n",
    "matrix = np.zeros((rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Create matrix representation of the train statements, X_matrix \n",
    "\n",
    "counts1 = 0 \n",
    "for statement in statements: \n",
    "    counts2 = 0\n",
    "    for word in vocab:\n",
    "        if word in statement:\n",
    "            count = statement.count(word)       #count how often the word occurs in the statement\n",
    "            matrix[counts1, counts2] = count    #puts number of occurences in the entry corresponding to that word\n",
    "        counts2 += 1 \n",
    "    counts1 += 1 \n",
    "\n",
    "print(matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create y vector which contains the validity labels of the statements in the train dataset \n",
    "labelsdic ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "size = len(statements)\n",
    "y_vector = [None] * size\n",
    "\n",
    "counter = 0\n",
    "for label in labels:\n",
    "    y_vector[counter] = labelsdic[label]  #convert the label to the corresponding integer\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will move on to applying multinomial linear regression to the matrix representation of the statements and the vector representation of the validity labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 3 0 3 4 1]\n"
     ]
    }
   ],
   "source": [
    "#delete this cell? \n",
    "\n",
    "lr = LogisticRegression(solver='lbfgs',multi_class='multinomial').fit(matrix, y_vector)\n",
    "yhat = lr.predict(matrix)\n",
    "print(yhat[:7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "lr.fit(matrix, y_vector) #apply linear regression to the statements matrix and labels vector of the train dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how our Linear Regression model is doing, we will need to test the model with the test dataset. In order to use the test dataset, we will first have to represent the data of this dataset in the same way as we did for the train dataset. As our linear regression model is based on the vocabulary of the train dataset, we will only use the words of the test dataset that occur in the vocabulary of the train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 statements: [['building', 'a', 'wall', 'on', 'the', 'usmexico', 'border', 'will', 'take', 'literally', 'year'], ['wisconsin', 'is', 'on', 'pace', 'to', 'double', 'the', 'number', 'of', 'layoff', 'this', 'year'], ['say', 'john', 'mccain', 'ha', 'done', 'nothing', 'to', 'help', 'the', 'vet']]\n",
      "First 3 labels: ['true', 'false', 'false']\n",
      "1267\n"
     ]
    }
   ],
   "source": [
    "#make list of statements for test dataset \n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "\n",
    "teststatements = []\n",
    "testlabels = []\n",
    "test_countlines = 0\n",
    "\n",
    "with open(\"test.tsv\", encoding=\"utf8\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        label = line[1]\n",
    "        aline = line[2]\n",
    "        bline = aline.split(\" \")\n",
    "        cline = []\n",
    "        test_countlines += 1 \n",
    "        for word in bline:\n",
    "            word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "            cline.append(word)\n",
    "        testlabels.append(label)\n",
    "        teststatements.append(cline)\n",
    "\n",
    "print(\"First 3 statements:\", teststatements[:3])\n",
    "print(\"First 3 labels:\", testlabels[:3])\n",
    "print(test_countlines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "rows = test_countlines                 #number of statements in the test dataset \n",
    "columns = len(vocab)                   #we will use the same vocabulary used in the train matrix\n",
    "testmatrix = np.zeros((rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Create a matrix representation for test dataset, X_matrix_test \n",
    "\n",
    "counts1 = 0 \n",
    "for statement in teststatements: \n",
    "    counts2 = 0\n",
    "    for word in vocab:              #we represent the test statements with respect to the words occuring in the train dataset\n",
    "        if word in statement:\n",
    "            count = statement.count(word)\n",
    "            testmatrix[counts1, counts2] = count\n",
    "        counts2 += 1 \n",
    "    counts1 += 1 \n",
    "\n",
    "print(testmatrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create test y vector\n",
    "labelsdic ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "size = len(teststatements)\n",
    "test_y_vector = [None] * size\n",
    "\n",
    "counter = 0\n",
    "for label in testlabels:\n",
    "    test_y_vector[counter] = labelsdic[label]\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x = testmatrix \n",
    "test_y = test_y_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now test the accuracy of the model on the statements of the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23835832675611682"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the accuracy is pretty low...\n",
    "We will now have look at the coefficiants that are given to the words of the vocabulary, for the six different validity labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.07262584  0.0415358   0.21254774 ... -0.30697811 -0.00765843\n",
      "  -0.05128795]\n",
      " [ 0.05065343 -0.00933911 -0.08832242 ... -0.05403901 -0.02912948\n",
      "  -0.07423348]\n",
      " [ 0.043321   -0.00971194 -0.05323927 ...  0.52651426 -0.15128071\n",
      "   0.25039853]\n",
      " [-0.09123454 -0.0497585  -0.01421984 ... -0.10734406 -0.03400038\n",
      "  -0.02533595]\n",
      " [-0.23978211  0.05981195 -0.04045164 ... -0.04471728 -0.01407327\n",
      "  -0.06120506]\n",
      " [ 0.16441637 -0.0325382  -0.01631457 ... -0.01343579  0.23614228\n",
      "  -0.03833609]]\n",
      "(6, 11916)\n",
      "#Rows is length of vocab:  11916\n"
     ]
    }
   ],
   "source": [
    "#double cell\n",
    "print(lr.coef_)\n",
    "print(lr.coef_.shape) #is of size (n_classes, n_features)\n",
    "print(\"#Columns is length of vocab: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_dict_false = dict()\n",
    "line0 = 0\n",
    "for word in vocab: \n",
    "    coef_dict_false[word] = lr.coef_[0][line0]\n",
    "    line0 += 1 \n",
    "    \n",
    "coef_dict_barely = dict()\n",
    "line1 = 0\n",
    "for word in vocab: \n",
    "    coef_dict_barely[word] = lr.coef_[1][line1]\n",
    "    line1 += 1 \n",
    "\n",
    "coef_dict_half = dict()\n",
    "line2 = 0\n",
    "for word in vocab: \n",
    "    coef_dict_half[word] = lr.coef_[2][line2]\n",
    "    line2 += 1 \n",
    "    \n",
    "coef_dict_mostly = dict()\n",
    "line3 = 0\n",
    "for word in vocab: \n",
    "    coef_dict_mostly[word] = lr.coef_[3][line3]\n",
    "    line3 += 1 \n",
    "    \n",
    "coef_dict_true = dict()\n",
    "line4 = 0\n",
    "for word in vocab: \n",
    "    coef_dict_true[word] = lr.coef_[4][line4]\n",
    "    line4 += 1 \n",
    "    \n",
    "coef_dict_pantsfire = dict()\n",
    "line5 = 0\n",
    "for word in vocab: \n",
    "    coef_dict_pantsfire[word] = lr.coef_[5][line5]\n",
    "    line5 += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('destroyed', 1.6392454228024622),\n",
       " ('taxed', 1.510072617819274),\n",
       " ('motor', 1.3773441327012856),\n",
       " ('responsibility', 1.348842040951605),\n",
       " ('scientist', 1.3059008288876481),\n",
       " ('125', 1.2903847356544436),\n",
       " ('supporter', 1.2579666985193203),\n",
       " ('cicilline', 1.2511152993137429),\n",
       " ('worked', 1.2383459057429123),\n",
       " ('scheme', 1.2347847219217412)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ordering\n",
    "from collections import OrderedDict\n",
    "\n",
    "ordered_false_coefs = [(k, coef_dict_false[k]) for k in sorted(coef_dict_false, key=coef_dict_false.get, reverse=True)]\n",
    "\n",
    "ordered_false_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('deciding', 1.3721003524964706),\n",
       " ('clear', 1.2633377552530909),\n",
       " ('benghazi', 1.2295733902314818),\n",
       " ('along', 1.2219629665519056),\n",
       " ('patient', 1.1995008134592766),\n",
       " ('3000', 1.1670839678753282),\n",
       " ('list', 1.143761250567385),\n",
       " ('illegals', 1.1426038552894828),\n",
       " ('key', 1.1386040499673395),\n",
       " ('easier', 1.1140354494265294)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_barely_coefs = [(k, coef_dict_barely[k]) for k in sorted(coef_dict_barely, key=coef_dict_barely.get, reverse=True)]\n",
    "ordered_barely_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('indiana', 1.7776271388910705),\n",
       " ('mammogram', 1.6280120819852053),\n",
       " ('deported', 1.4555018188393871),\n",
       " ('santorum', 1.429021410172006),\n",
       " ('ranking', 1.417563645580998),\n",
       " ('anytime', 1.3785708846276097),\n",
       " ('minnesota', 1.2654421919172874),\n",
       " ('confirmation', 1.250038968059138),\n",
       " ('direction', 1.2485952173938781),\n",
       " ('drink', 1.1789535137469767)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_half_coefs = [(k, coef_dict_half[k]) for k in sorted(coef_dict_half, key=coef_dict_half.get, reverse=True)]\n",
    "ordered_half_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('december', 1.5132457599814482),\n",
       " ('turn', 1.4807436902619637),\n",
       " ('earns', 1.3210329497753446),\n",
       " ('listed', 1.2870213918192295),\n",
       " ('mental', 1.274100476575609),\n",
       " ('doyles', 1.2578224068194608),\n",
       " ('79', 1.238493621558084),\n",
       " ('detail', 1.2064812845826935),\n",
       " ('graduating', 1.2061185647146462),\n",
       " ('fifty', 1.1966880256783268)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_mostly_coefs = [(k, coef_dict_mostly[k]) for k in sorted(coef_dict_mostly, key=coef_dict_mostly.get, reverse=True)]\n",
    "ordered_mostly_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blow', 1.551719663305779),\n",
       " ('heavily', 1.4751874594109746),\n",
       " ('compensation', 1.451694790330877),\n",
       " ('youve', 1.4207869222145146),\n",
       " ('block', 1.3861172774080845),\n",
       " ('affect', 1.3655152560188122),\n",
       " ('restrictive', 1.290167424392575),\n",
       " ('understand', 1.278986194651979),\n",
       " ('expense', 1.247373106301906),\n",
       " ('previously', 1.23030905327161)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_true_coefs = [(k, coef_dict_true[k]) for k in sorted(coef_dict_true, key=coef_dict_true.get, reverse=True)]\n",
    "ordered_true_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('face', 1.7550063199553634),\n",
       " ('socialist', 1.4979286829632277),\n",
       " ('takeover', 1.4660871623263878),\n",
       " ('either', 1.3953236816987387),\n",
       " ('rep', 1.389536641392727),\n",
       " ('navy', 1.3887019350030623),\n",
       " ('sic', 1.3410005192707595),\n",
       " ('cabinet', 1.3299437775151992),\n",
       " ('250000', 1.304883240300638),\n",
       " ('2016', 1.2965506308996206)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_pantsfire_coefs = [(k, coef_dict_pantsfire[k]) for k in sorted(coef_dict_pantsfire, key=coef_dict_pantsfire.get, reverse=True)]\n",
    "ordered_pantsfire_coefs[0:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
