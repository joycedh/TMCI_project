{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update 1 \n",
    "**Names:** Eva, Barbara & Joyce\n",
    "\n",
    "In this update we have created a vector representation of the training set, in which a count is given to the words in each statement according to which word from the vocabulary it is. This representation can possibly be used to decide which words/features in the data are the most important and should be given the most weight. However, we were not sure yet how to do that...\n",
    "\n",
    "\n",
    "Then, we tried some logistic regression on the data to predict which category (\"false\",\"barely-true\", \"half-true\", \"mostly-true\", \"true\", \"pants-fire\") a statement belongs to. In this logistic regression we did not use the above-mentioned matrix, because we don't know yet how to incorporate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in trainset: 184014\n",
      "Number of distinct words in trainset: 11916\n",
      "['say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand', 'when', 'did', 'the', 'decline', 'of', 'coal', 'start', 'it', 'started', 'when', 'natural', 'gas', 'took', 'off']\n",
      "['say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand', 'when', 'did', 'decline', 'of', 'coal', 'start', 'it', 'started', 'natural', 'gas', 'took', 'off', 'that', 'to']\n"
     ]
    }
   ],
   "source": [
    "#make a list of the vocabulary: all the distinct words occuring in the train set \n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import csv\n",
    "import string\n",
    "\n",
    "allwords = []\n",
    "with open(\"train.tsv\", encoding=\"utf8\") as tsvfile:    #open training set\n",
    "    lines = csv.reader(tsvfile, delimiter=\"\\t\")        #convert file to lines\n",
    "    for line in lines:\n",
    "        statement = line[2]                                #get statement from each line \n",
    "        lostrings = statement.split(\" \")                   #convert string to list of strings\n",
    "        new_lostrings = []\n",
    "        for word in lostrings:\n",
    "            word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "            new_lostrings.append(word)\n",
    "        allwords.extend(new_lostrings)\n",
    "\n",
    "vocab = []                          #initialize a list for all the distint words in the trainset\n",
    "for word in allwords:\n",
    "    if word in vocab:               #do not add word if word is already in vocabulary \n",
    "        continue \n",
    "    else:\n",
    "        vocab.append(word)\n",
    "\n",
    "\n",
    "print(\"Number of words in trainset:\", len(allwords))\n",
    "print(\"Number of distinct words in trainset:\", len(vocab))\n",
    "print(allwords[:25])\n",
    "print(vocab[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements: 10240\n",
      "First 3 statements: [['say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand'], ['when', 'did', 'the', 'decline', 'of', 'coal', 'start', 'it', 'started', 'when', 'natural', 'gas', 'took', 'off', 'that', 'started', 'to', 'begin', 'in', 'president', 'george', 'w', 'bush', 'administration'], ['hillary', 'clinton', 'agrees', 'with', 'john', 'mccain', 'by', 'voting', 'to', 'give', 'george', 'bush', 'the', 'benefit', 'of', 'the', 'doubt', 'on', 'iran']]\n"
     ]
    }
   ],
   "source": [
    "#make a list containing all the statements and count number of statements \n",
    "\n",
    "statements = []\n",
    "\n",
    "with open(\"train.tsv\", encoding=\"utf8\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        aline = line[2]\n",
    "        bline = aline.split(\" \")\n",
    "        cline = []\n",
    "        for word in bline:\n",
    "            word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "            cline.append(word)\n",
    "        statements.append(cline)\n",
    "\n",
    "\n",
    "countlines = len(statements)\n",
    "print(\"Number of statements:\", countlines)\n",
    "print(\"First 3 statements:\", statements[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of statements: 10240\n",
      "First 3 statements: [['false', 'say', 'the', 'annies', 'list', 'political', 'group', 'support', 'thirdtrimester', 'abortion', 'on', 'demand'], ['half-true', 'when', 'did', 'the', 'decline', 'of', 'coal', 'start', 'it', 'started', 'when', 'natural', 'gas', 'took', 'off', 'that', 'started', 'to', 'begin', 'in', 'president', 'george', 'w', 'bush', 'administration'], ['mostly-true', 'hillary', 'clinton', 'agrees', 'with', 'john', 'mccain', 'by', 'voting', 'to', 'give', 'george', 'bush', 'the', 'benefit', 'of', 'the', 'doubt', 'on', 'iran']]\n"
     ]
    }
   ],
   "source": [
    "#this list is similar to the previous list but it contains the label (true/false) at beginning of eacht statement \n",
    "statements = []\n",
    "\n",
    "with open(\"train.tsv\", encoding=\"utf8\") as tsvfile:\n",
    "    tsvreader = csv.reader(tsvfile, delimiter=\"\\t\")\n",
    "    for line in tsvreader:\n",
    "        aline = line[2]\n",
    "        label = line[1]\n",
    "        bline = aline.split(\" \")\n",
    "        cline = []\n",
    "        for word in bline:\n",
    "            word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation\n",
    "            cline.append(word)\n",
    "        cline.insert(0, label)\n",
    "        statements.append(cline)\n",
    "\n",
    "\n",
    "countlines = len(statements)\n",
    "print(\"Number of statements:\", countlines)\n",
    "print(\"First 3 statements:\", statements[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "rows = countlines\n",
    "columns = len(vocab)+1 #add one for the truth-value \n",
    "matrix = np.zeros((rows, columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  1. ...,  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# creating the afore-mentioned matrix representation\n",
    "counts1 = 0\n",
    "for statement in statements:\n",
    "    labelcount = labels[statement[0]] \n",
    "    matrix[counts1, 0] = labelcount\n",
    "    counts2 = 1\n",
    "    for word in vocab:\n",
    "        if word in statement:\n",
    "            count = statement.count(word)\n",
    "            matrix[counts1, counts2] = count\n",
    "        counts2 += 1 \n",
    "    counts1 += 1 \n",
    "\n",
    "print(matrix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  0.  0.  2.  1.]\n",
      " [ 2.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 3.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Testing using a smaller matrix\n",
    "\n",
    "labels ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4}\n",
    "lst = [\"words\", \"that\", \"can\", \"occur\", \"in\", \"a\",\"statement\"]\n",
    "example_statements =[[\"false\", \"a\", \"statement\", \"with\", \"words\", \"a\"], [\"half-true\", \"this\", \"is\", \"another\", \"example\", \"statement\"], \n",
    "                     [\"mostly-true\",\"and\",\"that\", \"was\", \"the\", \"last\", \"one\"]]\n",
    "\n",
    "example_matrix = np.zeros((len(example_statements), len(lst)+1))\n",
    "\n",
    "ex_counts1 = 0\n",
    "for statement in example_statements:\n",
    "    labelcount = labels[statement[0]] \n",
    "    example_matrix[ex_counts1, 0] = labelcount\n",
    "    ex_counts2 = 1\n",
    "    for word in lst:\n",
    "        if word in statement:\n",
    "            count = statement.count(word)\n",
    "            example_matrix[ex_counts1, ex_counts2] = count\n",
    "        ex_counts2 += 1 \n",
    "    ex_counts1 += 1 \n",
    "    \n",
    "\n",
    "print(example_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  2.  1.]\n",
      " [ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# different labeling \n",
    "\n",
    "labels ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "lst = [\"words\", \"that\", \"can\", \"occur\", \"in\", \"a\",\"statement\"]\n",
    "example_statements =[[\"false\", \"a\", \"statement\", \"with\", \"words\", \"a\"], [\"half-true\", \"this\", \"is\", \"another\", \"example\", \"statement\"], \n",
    "                     [\"mostly-true\",\"and\",\"that\", \"was\", \"the\", \"last\", \"one\"]]\n",
    "\n",
    "example_matrix = np.zeros((len(example_statements), len(lst)+6))\n",
    "\n",
    "ex_counts1 = 0\n",
    "for statement in example_statements:\n",
    "    labelcount = labels[statement[0]] \n",
    "    example_matrix[ex_counts1, labelcount] = 1\n",
    "    ex_counts2 = 6\n",
    "    for word in lst:\n",
    "        if word in statement:\n",
    "            count = statement.count(word)\n",
    "            example_matrix[ex_counts1, ex_counts2] = count\n",
    "        ex_counts2 += 1 \n",
    "    ex_counts1 += 1 \n",
    "    \n",
    "\n",
    "print(example_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "> In the previous 2 cells we have tested whether the matrix representation worked out. In the first cell you can see that it did, in the second we tried a different matrix representation. \n",
    "\n",
    "> Namely, in the second matrix, we represented the truth-value in a seperate column for each truth-value, with 0 if the statement doesn't have that truth value, and 1 if it does. We were thinking this might be better, because if you give the truth value a number, then 'pants-fire' might be more considered as a important feature, because it always counts as 5, whereas 'false' counts as 0... Not super sure about this, tough. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we do logistic regression on the data, similar to what we have done in lab 9: \"ML\". \n",
    "- We first created a dataframe from the data\n",
    "- Then created a new column in the df (\"class\"), representing the score for the different truth value. \n",
    "- Then made a count vector / TF-IDF representation of the statements as X and the belonging scores as y. \n",
    "- We then performed logistic regression, and tested it a bit filling in our own statements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os, codecs\n",
    "\n",
    "# create df\n",
    "df_liar = pd.read_csv(\"train.tsv\", encoding=\"utf8\", sep=\"\\t\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  truth-value                                               text  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                                topic            name                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state    politics  count1  count2  count3  count4  count5  \\\n",
       "0     Texas  republican     0.0     1.0     0.0     0.0     0.0   \n",
       "1  Virginia    democrat     0.0     0.0     1.0     1.0     0.0   \n",
       "2  Illinois    democrat    70.0    71.0   160.0   163.0     9.0   \n",
       "3       NaN        none     7.0    19.0     3.0     5.0    44.0   \n",
       "4   Florida    democrat    15.0     9.0    20.0    19.0     2.0   \n",
       "\n",
       "               context  \n",
       "0             a mailer  \n",
       "1      a floor speech.  \n",
       "2               Denver  \n",
       "3       a news release  \n",
       "4  an interview on CNN  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_liar.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "def classify(text):\n",
    "    return labels[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the class of truth-values\n",
    "df_liar[\"class\"] = df_liar[\"truth-value\"].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9028.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>The economic turnaround started at the end of ...</td>\n",
       "      <td>economy,jobs</td>\n",
       "      <td>charlie-crist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Florida</td>\n",
       "      <td>democrat</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>an interview on CNN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  truth-value                                               text  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "3   1123.json        false  Health care reform legislation is likely to ma...   \n",
       "4   9028.json    half-true  The economic turnaround started at the end of ...   \n",
       "\n",
       "                                topic            name                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "3                         health-care    blog-posting                   NaN   \n",
       "4                        economy,jobs   charlie-crist                   NaN   \n",
       "\n",
       "      state    politics  count1  count2  count3  count4  count5  \\\n",
       "0     Texas  republican     0.0     1.0     0.0     0.0     0.0   \n",
       "1  Virginia    democrat     0.0     0.0     1.0     1.0     0.0   \n",
       "2  Illinois    democrat    70.0    71.0   160.0   163.0     9.0   \n",
       "3       NaN        none     7.0    19.0     3.0     5.0    44.0   \n",
       "4   Florida    democrat    15.0     9.0    20.0    19.0     2.0   \n",
       "\n",
       "               context  class  \n",
       "0             a mailer      0  \n",
       "1      a floor speech.      2  \n",
       "2               Denver      3  \n",
       "3       a news release      0  \n",
       "4  an interview on CNN      2  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_liar.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10240, 12196)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF representation of the text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train = count_vect.fit_transform(df_liar.text)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subtract the classes\n",
    "y_train = df_liar[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction test 1: 0\n",
      "probability of belonging to classes: [ 0.48589017  0.06160173  0.14523659  0.06214997  0.08505282  0.16006872]\n",
      "prediction test 2: 3\n",
      "probability of belonging to classes: [ 0.26817292  0.04392243  0.1608359   0.34184172  0.04104249  0.14418455]\n",
      "prediction test 3: 2\n",
      "probability of belonging to classes: [ 0.14191871  0.06550172  0.28323085  0.24729368  0.19961711  0.06243794]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "test1 = \"Joyce is becoming the new president of America\"\n",
    "test2 = \"Barbara just hijacked a plane! OMG\"\n",
    "test3 = \"Eva wants to build a wall between the Netherlands and Belgium!\"\n",
    "\n",
    "x = count_vect.transform([test1])\n",
    "x1 = count_vect.transform([test2])\n",
    "x2 = count_vect.transform([test3])\n",
    "print(\"prediction test 1:\", logreg.predict(x)[0])\n",
    "print(\"probability of belonging to classes:\", logreg.predict_proba(x)[0])\n",
    "\n",
    "print(\"prediction test 2:\", logreg.predict(x1)[0])\n",
    "print(\"probability of belonging to classes:\", logreg.predict_proba(x1)[0])\n",
    "\n",
    "print(\"prediction test 3:\", logreg.predict(x2)[0])\n",
    "print(\"probability of belonging to classes:\", logreg.predict_proba(x2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "- We tried to evaluate the model, but because the pre-defined train and test set do not have the same vocabulary, they cannot be used to test... Or maybe in a way that we don't know yet. Below we have provided a test by just splitting up the training X into a train and test set, fixing this problem, but not really. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1267, 4348)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_liar_test = pd.read_csv(\"test.tsv\", encoding=\"utf8\", sep=\"\\t\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\"])\n",
    "df_liar_test[\"class\"] = df_liar_test[\"truth-value\"].apply(classify)\n",
    "\n",
    "X_test = count_vect.fit_transform(df_liar_test.text)\n",
    "y_test = df_liar_test[\"class\"].values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 4348 features per sample; expecting 12196",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-11c84e9dd63e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# we could try to evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my_hat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# evaluate using accuracy: proportion of correctly predicted over total\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \"\"\"\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             raise ValueError(\"X has %d features per sample; expecting %d\"\n\u001b[0;32m--> 270\u001b[0;31m                              % (X.shape[1], n_features))\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         scores = safe_sparse_dot(X, self.coef_.T,\n",
      "\u001b[0;31mValueError\u001b[0m: X has 4348 features per sample; expecting 12196"
     ]
    }
   ],
   "source": [
    "# we could try to evaluate the model\n",
    "logreg.fit(X_train, y_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# splitting the matrices we used for training. \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "X_train2, X_val, y_train2, y_val = train_test_split(X_train2, y_train2, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21875\n",
      "672\n"
     ]
    }
   ],
   "source": [
    "# evaluating again\n",
    "logreg.fit(X_train2, y_train2)\n",
    "y_hat_test2 = logreg.predict(X_test2)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test2, y_hat_test2))\n",
    "print(accuracy_score(y_test2, y_hat_test2, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#multinomial logistic regression\n",
    "import numpy as np\n",
    "\n",
    "Xtrain = np.array([[1,    20,   30,   1],\n",
    "                   [2,    22,   12,   33],\n",
    "                   [3,    45,   65,   77],\n",
    "                   [12,   43,   55,   65],\n",
    "                   [11,   25,   30,   1],\n",
    "                   [22,   23,   19,   31],\n",
    "                   [31,   41,   11,   70],\n",
    "                   [1,    48,   23,   60]])\n",
    "\n",
    "ytrain = np.array([1, 2, 3, 4, 1, 2, 3, 4])\n",
    "lr = LogisticRegression(solver='lbfgs',multi_class='multinomial').fit(Xtrain, ytrain)\n",
    "yhat = lr.predict(Xtrain)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "This model is probably too small to provide accurate results, since the training set is now downgraded a lot... Therefore it has super low accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
