{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression on Liar Dataset\n",
    "** Names: ** Barbara, Eva & Joyce\n",
    "<br><br>\n",
    "In this notebook we will implement multinomial logistic regression in various different ways, to see what the best multinomial model is. As we will include sentiment scores later on, we will use this dataset from the start rather than just the normal train dataset. This dataset includes all the information of the normal liar dataset, but includes the sentiment scores for each statement. After preprocessing the Liar Dataset we will apply Logistic Regression. We will first do this by using Count Vectorizer to represent the data of our train and test dataset. We will then use TF-IDF Vectorizer to represent our data and implement Logistic Regression. Once we have these two different logistic regression models, we reduce the dimensionality of the data by using SVD and apply this to our (so-far) best-performing model. Then, apply filtering of words in the statements and then test this on our best performing model. After doing this we will add the sentiment score and political score as features to our LR model. At the end of the notebook we will show whether these new features have any impact on the regression at all. Finally, we will present an overview of the accuracies of our different multinomial models.\n",
    "\n",
    "### Index\n",
    "1. ** Preprocessing Liar dataset ** (adding sentiment + political score)\n",
    "2. ** Multinomial LR using `Count Vectorizer`**\n",
    "    - regression\n",
    "    - feature importance \n",
    "3. ** Multinomial LR with `TF-IDF Vectorizer`** (on data without sentiment & political score)\n",
    "    - non SVD \n",
    "    - with SVD \n",
    "    \n",
    "4. ** Filtering ** \n",
    "    - non SVD TF-IDF model (since this is the best performing model)\n",
    "   \n",
    "5. ** Implementing more features: sentiment & political score ** (on best multinomial model)\n",
    "    - regression\n",
    "    - feature importance\n",
    "\n",
    "6. ** Final results & remarks ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from sklearn import decomposition\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10240, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open the csv file.\n",
    "df_liar_sentiment = pd.read_csv(\"sentiment_train.csv\", encoding=\"utf8\", sep=\",\")\n",
    "df_liar_sentiment.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Adding truth scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scoring metric for the different truthvalues \n",
    "truthlabels ={\"false\":0, \"barely-true\":1,\"half-true\":2,\"mostly-true\":3,\"true\":4, \"pants-fire\":5}\n",
    "\n",
    "# classification formula for truth value\n",
    "def classify_truth(text):\n",
    "    if text not in truthlabels.keys(): \n",
    "        return -1\n",
    "    else:\n",
    "        return truthlabels[text]\n",
    "\n",
    "# add this new class of truth scores\n",
    "df_liar_sentiment[\"truth-score\"] = df_liar_sentiment[\"truth-value\"].apply(classify_truth) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Adding political scoring\n",
    "Since there are quite a lot of political labels in the dataset, we have focussed to only include the 3 labels that are the most occuring in the dataset: 'republican', 'democrat' and 'none'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'republican': 4497, 'democrat': 3336, 'none': 1744, 'organization': 219, 'independent': 147, 'columnist': 35, 'activist': 39, 'talk-show-host': 26, 'libertarian': 40, 'newsmaker': 56, 'journalist': 38, 'labor-leader': 11, 'state-official': 20, 'business-leader': 9, 'education-official': 2, 'tea-party-member': 10, nan: 2, 'green': 3, 'liberal-party-canada': 1, 'government-body': 1, 'Moderate': 1, 'democratic-farmer-labor': 1, 'ocean-state-tea-party-action': 1, 'constitution-party': 1}\n"
     ]
    }
   ],
   "source": [
    "# let's see how many political preferences there are\n",
    "politics = dict()\n",
    "for line in df_liar_sentiment[\"politics\"]:\n",
    "    if line not in politics.keys():\n",
    "        politics[line] = 1\n",
    "    else:\n",
    "        politics[line] +=1\n",
    "print(politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll only focus on the 3 most occuring political preferences\n",
    "politics = dict({\"republican\": 0, \"democrat\" : 1, \"none\" : 2})\n",
    "\n",
    "# classification formula for political background\n",
    "def classify_politics(text):\n",
    "    if text not in politics.keys(): \n",
    "        return -1\n",
    "    else:\n",
    "        return politics[text]\n",
    "\n",
    "# add this new class of politic scores\n",
    "df_liar_sentiment[\"political-score\"] = df_liar_sentiment[\"politics\"].apply(classify_politics) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9577, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "      <th>pos-sentiment</th>\n",
       "      <th>neg-sentiment</th>\n",
       "      <th>truth-score</th>\n",
       "      <th>political-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id truth-value                                               text  \\\n",
       "0  2635.json       false  Says the Annies List political group supports ...   \n",
       "\n",
       "      topic          name                   job  state    politics  count1  \\\n",
       "0  abortion  dwayne-bohac  State representative  Texas  republican     0.0   \n",
       "\n",
       "   count2  count3  count4  count5   context  pos-sentiment  neg-sentiment  \\\n",
       "0     1.0     0.0     0.0     0.0  a mailer       0.007972       0.012908   \n",
       "\n",
       "   truth-score  political-score  \n",
       "0            0                0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtering out the statement with a truth-score / a political preference that we don't like\n",
    "df_reduced_sent1 = df_liar_sentiment[df_liar_sentiment[\"truth-score\"] != -1]\n",
    "df_reduced_sent2 = df_reduced_sent1[df_reduced_sent1[\"political-score\"] != -1]\n",
    "print(df_reduced_sent2.shape)\n",
    "df_reduced_sent2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>context</th>\n",
       "      <th>pos-sentiment</th>\n",
       "      <th>neg-sentiment</th>\n",
       "      <th>truth-score</th>\n",
       "      <th>political-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  truth-value                                               text     topic  \\\n",
       "0       false  Says the Annies List political group supports ...  abortion   \n",
       "\n",
       "           name                   job  state    politics   context  \\\n",
       "0  dwayne-bohac  State representative  Texas  republican  a mailer   \n",
       "\n",
       "   pos-sentiment  neg-sentiment  truth-score  political-score  \n",
       "0       0.007972       0.012908            0                0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reducing the dataframe to only the important/most interesting data\n",
    "df_reduced_sent = df_reduced_sent2.drop(['id', 'count1', 'count2', 'count3', 'count4', 'count5'], axis=1)  \n",
    "df_reduced_sent.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Doing the same for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df of the test data\n",
    "df_test_sent = pd.read_csv(\"sentiment_test.csv\", encoding=\"utf8\", sep=\",\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\", \n",
    "                                                                                        \"pos-sentiment\", \"neg-sentiment\"])\n",
    "df_test_sent[\"truth-score\"] = df_test_sent[\"truth-value\"].apply(classify_truth)\n",
    "df_test_sent[\"political-score\"] = df_test_sent[\"politics\"].apply(classify_politics)\n",
    "\n",
    "df_test_sent_reduced1 = df_test_sent[df_test_sent[\"truth-score\"] != -1]\n",
    "df_test_sent_reduced2 = df_test_sent_reduced1[df_test_sent_reduced1[\"political-score\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>context</th>\n",
       "      <th>pos-sentiment</th>\n",
       "      <th>neg-sentiment</th>\n",
       "      <th>truth-score</th>\n",
       "      <th>political-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>true</td>\n",
       "      <td>Building a wall on the U.S.-Mexico border will...</td>\n",
       "      <td>immigration</td>\n",
       "      <td>rick-perry</td>\n",
       "      <td>Governor</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>Radio interview</td>\n",
       "      <td>0.007971938775510204</td>\n",
       "      <td>0.012908163265306121</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  truth-value                                               text        topic  \\\n",
       "1        true  Building a wall on the U.S.-Mexico border will...  immigration   \n",
       "\n",
       "         name       job  state    politics          context  \\\n",
       "1  rick-perry  Governor  Texas  republican  Radio interview   \n",
       "\n",
       "          pos-sentiment         neg-sentiment  truth-score  political-score  \n",
       "1  0.007971938775510204  0.012908163265306121            4                0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_sent_reduced = df_test_sent_reduced2.drop(['id', 'count1', 'count2', 'count3', 'count4', 'count5'], axis=1)  \n",
    "df_test_sent_reduced.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  Regression using CountVec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. *Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_cv = count_vect.fit(df_reduced_sent.text)          #our X matrix is the text from the statements \n",
    "X_train_cv = count_vect.transform(df_reduced_sent.text)    \n",
    "\n",
    "y_train_cv = df_reduced_sent[\"truth-score\"].values         #our y vector is the list of all the truth labels     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs', multi_class='multinomial')\n",
    "logreg.fit(X_train_cv, y_train_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cv = count_vect.transform(df_test_sent_reduced.text)        # X matrix is again text from statements\n",
    "y_test_cv = df_test_sent_reduced[\"truth-score\"].values             # y vector is list of all the truth values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2434928631402183\n",
      "290\n"
     ]
    }
   ],
   "source": [
    "y_hat_test_cv = logreg.predict(X_test_cv)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test_cv, y_hat_test_cv))\n",
    "print(accuracy_score(y_test_cv, y_hat_test_cv, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "> The accuracy of this multinomial model is unfortunately very low. However, we expected the accuracy to be low, as the accuracy of the binomial model was only around 60%. As there are 6 different labels for validity in this case with some labels between false and true the difference between the statements is smaller than in the case of only true and false and thus probably more difficult to distinguish. We saw from the binomial model that it was already difficult to predict the label of the false and true statements.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. *Feature Importance*\n",
    "Here we create dictionaries for all the features, which are all the different words of the train dataset, with their corresponding weight in this logistic regression model. All the words have a different weight for the six different labels. We therefor have six dictionaries with all the words and their importance weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.07141639  0.00501307 -0.04506177 ... -0.04890659 -0.02738769\n",
      "  -0.06304054]\n",
      " [-0.0401037  -0.09912629  0.23734855 ... -0.01895194  0.31864874\n",
      "  -0.25438965]\n",
      " [-0.11112725  0.21189963 -0.03978087 ... -0.11737302 -0.07922386\n",
      "  -0.04646419]\n",
      " [-0.01736026  0.33066743 -0.07158738 ...  0.34770544 -0.15955491\n",
      "  -0.14338132]\n",
      " [-0.0070142  -0.02009042 -0.06017099 ... -0.13507738 -0.04778029\n",
      "  -0.18076825]\n",
      " [ 0.2470218  -0.42836342 -0.02074755 ... -0.02739652 -0.00470198\n",
      "   0.68804395]]\n",
      "(6, 11765)\n"
     ]
    }
   ],
   "source": [
    "print(logreg.coef_)\n",
    "print(logreg.coef_.shape)\n",
    "# This coefficient matrix has 6 rows as there are 6 different labels. \n",
    "#For every label each word has a different weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coefficient dictionaries for the different labels\n",
    "\n",
    "#label false \n",
    "coef_dict1_false = dict()                                    \n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict1_false[key] = logreg.coef_[0][n] \n",
    "    \n",
    "#label barely-true\n",
    "coef_dict1_barelytrue = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict1_barelytrue[key] = logreg.coef_[1][n] \n",
    "\n",
    "#label half-true\n",
    "coef_dict1_halftrue = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict1_halftrue[key] = logreg.coef_[2][n] \n",
    "    \n",
    "#label mostly-true\n",
    "coef_dict1_mostlytrue = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict1_mostlytrue[key] = logreg.coef_[3][n] \n",
    "    \n",
    "#label true\n",
    "coef_dict1_true = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict1_true[key] = logreg.coef_[4][n] \n",
    "    \n",
    "#label pants-fire \n",
    "coef_dict1_pantsfire = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict1_pantsfire[key] = logreg.coef_[5][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordering the different coefficient dictionaries\n",
    "\n",
    "ordered_coefs_false = [(k, coef_dict1_false[k]) for k in sorted(coef_dict1_false, key=coef_dict1_false.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_barelytrue = [(k, coef_dict1_barelytrue[k]) for k in sorted(coef_dict1_barelytrue, key=coef_dict1_barelytrue.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_halftrue = [(k, coef_dict1_halftrue[k]) for k in sorted(coef_dict1_halftrue, key=coef_dict1_halftrue.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_mostlytrue = [(k, coef_dict1_mostlytrue[k]) for k in sorted(coef_dict1_mostlytrue, key=coef_dict1_mostlytrue.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_true = [(k, coef_dict1_true[k]) for k in sorted(coef_dict1_true, key=coef_dict1_true.get, reverse=True)]\n",
    "\n",
    "ordered_coefs_pantsfire = [(k, coef_dict1_pantsfire[k]) for k in sorted(coef_dict1_pantsfire, key=coef_dict1_pantsfire.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reasonable', 1.551299571555561),\n",
       " ('authored', 1.4732789953865582),\n",
       " ('verifies', 1.3096547758452828),\n",
       " ('concludes', 1.2655602916155169),\n",
       " ('guards', 1.2114423464254092),\n",
       " ('dioxide', 1.206493037160284),\n",
       " ('ira', 1.2055905251486978),\n",
       " ('spill', 1.19673027826057),\n",
       " ('farouk', 1.1833886135054632),\n",
       " ('rihanna', 1.1783243697466481)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_false[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('drop', 1.3007233358230523),\n",
       " ('offender', 1.2934828071725997),\n",
       " ('airports', 1.2718334283154786),\n",
       " ('identity', 1.2370813286008355),\n",
       " ('heidi', 1.2303936122788284),\n",
       " ('raise', 1.2295257720098194),\n",
       " ('weakening', 1.1762635684278318),\n",
       " ('webb', 1.1716577894897826),\n",
       " ('regularly', 1.1669798094247215),\n",
       " ('package', 1.158812276538333)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_barelytrue[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('steady', 1.5143594902381408),\n",
       " ('crushes', 1.5054199700817747),\n",
       " ('cato', 1.4872095839764192),\n",
       " ('himself', 1.3580486698153738),\n",
       " ('11023', 1.2894362510873283),\n",
       " ('2021', 1.2448034686282377),\n",
       " ('wilson', 1.2268877620782388),\n",
       " ('indefinitely', 1.202461399199106),\n",
       " ('stadium', 1.2008768750511016),\n",
       " ('midwestern', 1.1989869225218057)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_halftrue[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('concept', 1.8823921133934578),\n",
       " ('oscar', 1.798451391812844),\n",
       " ('stimulate', 1.6314212493249507),\n",
       " ('stands', 1.4542596366590907),\n",
       " ('seeking', 1.2675739020164032),\n",
       " ('thousand', 1.224958779688441),\n",
       " ('weems', 1.1937898948326273),\n",
       " ('amending', 1.1647361671150147),\n",
       " ('focused', 1.1640392201365017),\n",
       " ('favor', 1.1407078796881214)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_mostlytrue[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arent', 1.5985036509803325),\n",
       " ('create', 1.407444839586859),\n",
       " ('thompson', 1.3721746625624904),\n",
       " ('burdensome', 1.2948823119855133),\n",
       " ('hated', 1.2527226155427307),\n",
       " ('calderon', 1.2409849744771508),\n",
       " ('sayspeter', 1.2004616692205996),\n",
       " ('projects', 1.1673800329571766),\n",
       " ('earthquake', 1.1385532376865188),\n",
       " ('directed', 1.1379934450253566)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_true[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alliance', 1.8405279368534364),\n",
       " ('avowed', 1.4767565631882693),\n",
       " ('jv', 1.453418944083574),\n",
       " ('phds', 1.4309000472424644),\n",
       " ('greensboro', 1.4211868545478252),\n",
       " ('probationers', 1.3432274525330101),\n",
       " ('khrushchev', 1.3238150037630398),\n",
       " ('promptly', 1.3120623369091902),\n",
       " ('internationally', 1.2939867159179281),\n",
       " ('adjust', 1.2744458387466617)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_coefs_pantsfire[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    ">Here we can see which words, for each label, were important in determining its label. Interestingly, the words with the highest weigt for \"true\", are different from the words with the highest weight in binomial logistic regression. Furthermore, the words with the lowest weight in the binomial logistic regression, which were the words contributed most to labeling the statement as false, did not correspond with the highest weights of the \"false\" label. As there are multiple labels in this case, the whole model is ofcourse different, however we would have expected some simmilarities. Furthermore some of the words with the highest weights seem very strange to actually be of importance in determining the label of the statement such as \"jv\" and \"11023\". However, as our model has very low accuracy, we cannot really extract useful information from these coefficient matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Regression using TF-IDF Vectorizer\n",
    "Here, we will do a Logistic Regression using the TF-IDF vectorizer, but not yet taking into acount the sentiment/political scorings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9577, 11765)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# creating the training vector\n",
    "X_train = tfidf_vect.fit(df_reduced_sent.text)\n",
    "X_train = tfidf_vect.transform(df_reduced_sent.text)\n",
    "y_train = df_reduced_sent[\"truth-score\"].values\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver='lbfgs',multi_class='multinomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1191, 11765)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the test data to the right format, aligning with the training data \n",
    "# (so that it has the size of the vocab of the training set)\n",
    "X_test = tfidf_vect.transform(df_test_sent_reduced.text) \n",
    "y_test = df_test_sent_reduced[\"truth-score\"].values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2510495382031906\n",
      "299\n"
     ]
    }
   ],
   "source": [
    "# evaluating the tfidf model\n",
    "lr.fit(X_train, y_train)\n",
    "y_hat_test = lr.predict(X_test)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "> The accuracy is not that much higher in comparison with the countvectorizer LR from the other notebook (0.245 vs. 0.251). That's kind of disappointing, but we did expect it since for binomial regression it also wasn't that much better. \n",
    "\n",
    "##### 2. Let's try dimensionality reduction\n",
    "Since the Tfidf model gives the highest accuracy we will apply SVD to this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# converting train to a matrix\n",
    "new_train = np.empty([9577, 11765])\n",
    "array_X_train = X_train.toarray()\n",
    "\n",
    "for n in range(9577):\n",
    "    new_train[n] = array_X_train[n]\n",
    "\n",
    "# converting test to a matrix\n",
    "new_test = np.empty([1191, 11765])\n",
    "array_X_test = X_test.toarray()\n",
    "\n",
    "for n in range(1191):\n",
    "    new_test[n] = array_X_test[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 50 dimensions\n",
    "train_SVD50Mat = decomposition.TruncatedSVD(n_components = 50, algorithm = \"arpack\").fit_transform(new_train)\n",
    "test_SVD50Mat = decomposition.TruncatedSVD(n_components = 50, algorithm = \"arpack\").fit_transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.172124265323\n",
      "205\n"
     ]
    }
   ],
   "source": [
    "# evaluating 50 dimensions SVD model\n",
    "lr.fit(train_SVD50Mat, y_train)\n",
    "y_hat_test_SVD = lr.predict(test_SVD50Mat)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test_SVD))\n",
    "print(accuracy_score(y_test, y_hat_test_SVD, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "> Again, as we also saw in the binomial regression, after SVD it doesn't give better results than normal, so there's no point for implementing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filtering \n",
    "Here we will lemmatize all words, filter out numbers and stop words. After applying the filtering to the text of the statements, we can see if our LR model will iprove by including filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords \n",
    "import string \n",
    "# preprocessing function to lemmatize and to filter out unimportant words. \n",
    "def filtering(text):\n",
    "    lostrings = text.split(' ')\n",
    "    new_lostrings = []\n",
    "    for word in lostrings:\n",
    "        word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "        if word not in stopwords.words('english') and not word.isdigit(): # remove stopwords & digits\n",
    "            new_lostrings.append(word)\n",
    "    return ' '.join(new_lostrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced_sent[\"filteredtext\"] = df_reduced_sent[\"text\"].apply(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sent_reduced[\"filteredtext\"] = df_test_sent_reduced[\"text\"].apply(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9577, 10667) (1191, 10667)\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# creating the training vector\n",
    "X_train_filter = tfidf_vect.fit(df_reduced_sent.filteredtext)\n",
    "X_train_filter = tfidf_vect.transform(df_reduced_sent.filteredtext)\n",
    "\n",
    "# creating the test vector\n",
    "X_test_filter = tfidf_vect.transform(df_test_sent_reduced.filteredtext) \n",
    "\n",
    "print(X_train_filter.shape, X_test_filter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24769101595298068\n",
      "295\n"
     ]
    }
   ],
   "source": [
    "# evaluating the filtered tfidf model\n",
    "\n",
    "lr.fit(X_train_filter, y_train)\n",
    "y_hat_test = lr.predict(X_test_filter)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    ">Opposed to what we expected, applying filtering and lemmatization does not seem to improve our model (0.248 vs 0.251). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementing more features: politics & sentiment\n",
    "Now, it's finally time to try to implement the new scorings that we added to our data. We will perform the same regression as above, but this time adding 3 new colums signifying the positive score, the negative score and the political score of the statement respectively.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Adding the new columns to the train and test matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.00797194  0.01290816  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.01148072  0.0142225   1.        ]\n",
      " [ 0.          0.          0.         ...,  0.00925808  0.01196854  1.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.00999878  0.01135029  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.00741618  0.01049563  1.        ]\n",
      " [ 0.          0.          0.         ...,  0.01075098  0.00883072  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we need to add 3 new columns: one for the positive sentiment score, \n",
    "# one for the negative score and one for the political score. \n",
    "X_train2 = np.empty([9577, 11768])\n",
    "\n",
    "for n in range(9577):\n",
    "    pos = df_reduced_sent[\"pos-sentiment\"].values[n]\n",
    "    neg = df_reduced_sent[\"neg-sentiment\"].values[n]\n",
    "    pol = df_reduced_sent[\"political-score\"].values[n]\n",
    "    X_train2[n] = np.append(new_train[n], [pos, neg, pol])\n",
    "\n",
    "print(X_train2)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.00797194  0.01290816  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.01148072  0.0142225   1.        ]\n",
      " [ 0.          0.          0.         ...,  0.00925808  0.01196854  0.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.00970238  0.01297619  1.        ]\n",
      " [ 0.          0.          0.         ...,  0.00965795  0.00716801  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.00975765  0.01259566  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we also need to do this for the test set.\n",
    "X_test2 = np.empty([1191, 11768])\n",
    "\n",
    "for n in range(1191):\n",
    "    pos = df_test_sent_reduced[\"pos-sentiment\"].values[n]\n",
    "    neg = df_test_sent_reduced[\"neg-sentiment\"].values[n]\n",
    "    pol = df_test_sent_reduced[\"political-score\"].values[n]\n",
    "    X_test2[n] = np.append(new_test[n], [pos, neg, pol])\n",
    "\n",
    "print(X_test2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Evaluation / regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.252728799328\n",
      "301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# evaluating the new model\n",
    "lr.fit(X_train2, y_train)\n",
    "y_hat_test = lr.predict(X_test2)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "> Here you will see that it only classifies 2 more test statements correctly in comparison with our previous model (with accuracy 0.251 vs. 0.253). Therefore, we don't really think the sentiment scoring and the political scoring have any effect on the results, but it was at least interesting to try..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Feature importance\n",
    "We also wanted to know *how* significant our new features were in the regression. Therefore, we evaluated there feature importance below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.04752767 -0.41708012 -0.0371028  ..., -0.07067812 -0.27048791\n",
      "  -0.15765825]\n",
      " [-0.03456917 -0.2710325   0.17343884 ...,  0.00767341  0.07877831\n",
      "  -0.15604081]\n",
      " [-0.06063145  0.66533971 -0.03342733 ..., -0.02959719  0.09569888\n",
      "  -0.0153626 ]\n",
      " [-0.03132313  1.26773272 -0.0475349  ...,  0.12738169  0.19015447\n",
      "   0.10484853]\n",
      " [-0.01839588 -0.20426712 -0.03560098 ..., -0.03611805  0.02074004\n",
      "   0.00946544]\n",
      " [ 0.1924473  -1.0406927  -0.01977283 ...,  0.00133826 -0.11488379\n",
      "   0.21474769]]\n",
      "(6, 11768)\n"
     ]
    }
   ],
   "source": [
    "# print out the coefficients \n",
    "print(lr.coef_)\n",
    "print(lr.coef_.shape) #is of size (n_classes, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding the names of the newly added columns to the vocabulary of our features\n",
    "features = []\n",
    "for key in tfidf_vect.vocabulary_.keys():\n",
    "    features.append(key)\n",
    "\n",
    "features = features + [\"positive-score\", \"negative-score\", \"political-score\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating a dictionary of the coefficient scores of each feature, for each truth-value label\n",
    "coef_dict_false = dict()\n",
    "start = 0\n",
    "for feature in features: \n",
    "    coef_dict_false[feature] = lr.coef_[0][start]\n",
    "    start += 1 \n",
    "    \n",
    "coef_dict_barely = dict()\n",
    "start = 0\n",
    "for feature in features: \n",
    "    coef_dict_barely[feature] = lr.coef_[1][start]\n",
    "    start += 1 \n",
    "\n",
    "coef_dict_half = dict()\n",
    "start = 0\n",
    "for feature in features: \n",
    "    coef_dict_half[feature] = lr.coef_[2][start]\n",
    "    start += 1 \n",
    "    \n",
    "coef_dict_mostly = dict()\n",
    "start = 0\n",
    "for feature in features: \n",
    "    coef_dict_mostly[feature] = lr.coef_[3][start]\n",
    "    start += 1 \n",
    "    \n",
    "coef_dict_true = dict()\n",
    "start = 0\n",
    "for feature in features: \n",
    "    coef_dict_true[feature] = lr.coef_[4][start]\n",
    "    start += 1 \n",
    "    \n",
    "coef_dict_pantsfire = dict()\n",
    "start = 0\n",
    "for feature in features: \n",
    "    coef_dict_pantsfire[feature] = lr.coef_[5][start]\n",
    "    start += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('campbell', 1.5514009372940964),\n",
       " ('reveal', 1.4204777899958099),\n",
       " ('vetoing', 1.3378204600073866),\n",
       " ('160', 1.3333355222925962),\n",
       " ('dioxide', 1.2893928312562191),\n",
       " ('authored', 1.2651959516125),\n",
       " ('karen', 1.2618004549643798),\n",
       " ('fsas', 1.2395506990394158),\n",
       " ('spill', 1.2237527964892905),\n",
       " ('bureaus', 1.223451243322266)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most important features determining the \"false\" labeled statements \n",
    "ordered_false_coefs = [(k, coef_dict_false[k]) for k in sorted(coef_dict_false, key=coef_dict_false.get, reverse=True)]\n",
    "ordered_false_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('regularly', 1.4891080350411461),\n",
       " ('safer', 1.2653403063840096),\n",
       " ('identity', 1.2348181864638454),\n",
       " ('topics', 1.2292261122743249),\n",
       " ('offender', 1.2274607718830501),\n",
       " ('package', 1.189296131791274),\n",
       " ('wastes', 1.133781841091499),\n",
       " ('thatcher', 1.1283452001889964),\n",
       " ('lazy', 1.1278096347111295),\n",
       " ('drop', 1.1199054660037748)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most important features determining the \"barely true\" labeled statements \n",
    "ordered_barely_coefs = [(k, coef_dict_barely[k]) for k in sorted(coef_dict_barely, key=coef_dict_barely.get, reverse=True)]\n",
    "ordered_barely_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments \n",
    "> As you can see here, our new features do not really occur in the top-10 important features for our labels, so therefore we decided to just check per label what their coefficient scores are, as seen below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Coefficient scores per new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative scoring coefficient for false : -0.0706781212921\n",
      "negative scoring coefficient for barely-true : 0.00767340941101\n",
      "negative scoring coefficient for half-true : -0.0295971886476\n",
      "negative scoring coefficient for mostly-true : 0.127381687191\n",
      "negative scoring coefficient for true : -0.0361180504101\n",
      "negative scoring coefficient for pants-fire : 0.00133826374786\n"
     ]
    }
   ],
   "source": [
    "# let's see what the importance is of the negative score for all labels. \n",
    "labellist = [\"false\", \"barely-true\",\"half-true\",\"mostly-true\",\"true\", \"pants-fire\"]\n",
    "\n",
    "for n, line in enumerate(lr.coef_):\n",
    "    print(\"negative scoring coefficient for\", labellist[n], \":\", line[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive scoring coefficient for false : -0.27048790946\n",
      "positive scoring coefficient for barely-true : 0.0787783058196\n",
      "positive scoring coefficient for half-true : 0.095698883791\n",
      "positive scoring coefficient for mostly-true : 0.190154473183\n",
      "positive scoring coefficient for true : 0.0207400362779\n",
      "positive scoring coefficient for pants-fire : -0.114883789612\n"
     ]
    }
   ],
   "source": [
    "# let's see what the importance is of the positive score for all labels. \n",
    "for n, line in enumerate(lr.coef_):\n",
    "    print(\"positive scoring coefficient for\", labellist[n], \":\", line[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "political scoring coefficient for false : -0.157658250775\n",
      "political scoring coefficient for barely-true : -0.156040812142\n",
      "political scoring coefficient for half-true : -0.0153626006286\n",
      "political scoring coefficient for mostly-true : 0.104848530231\n",
      "political scoring coefficient for true : 0.00946543984909\n",
      "political scoring coefficient for pants-fire : 0.214747693466\n"
     ]
    }
   ],
   "source": [
    "# let's see what the importance is of the political score for all labels. \n",
    "for n, line in enumerate(lr.coef_):\n",
    "    print(\"political scoring coefficient for\", labellist[n], \":\", line[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "> We see that the positive score has the highest coefficient value in the labeling of \"mostly true\" (giving coefficient = 0.127). For the negative score this is in labeling of \"mostly true\" too (coefficient = 0.190). And for political score this is in \"mostly true\" (coefficient = 0.105) and \"pants on fire\" (coefficient = 0.215). However, in comparison with the most important features of these labels, these coefficient scores are still not really significant, since they all score far below 1 still."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Final results & remarks\n",
    "Here are the overall results, nicely presented in a table. So in conclusion, our model *with* the new sentiment and political features works the best, but it doesn't show too much of a difference with the normal tfidf model that we already had.\n",
    "\n",
    "Model | Accuracy \n",
    "--- | --- \n",
    "nonSVDCountvec | 0.243492\n",
    "nonSVDtfidf | 0.25105\n",
    "SVD50tfidf | 0.17212\n",
    "nonSVDtfidf + filter | 0.24769\n",
    "nonSVDtfidf + new features | 0.25273"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
