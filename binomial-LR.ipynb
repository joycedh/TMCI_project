{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binomial Logistic Regression\n",
    "\n",
    "In this notebook, you will find a binomial regression done on a reduced part of the training set (i.e. we took out all the statements that were not labeled 'false' or 'true'). We created this notebook to get ourself acquainted with how logistic regression and feature importance works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import decomposition\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10540.json</td>\n",
       "      <td>half-true</td>\n",
       "      <td>When did the decline of coal start? It started...</td>\n",
       "      <td>energy,history,job-accomplishments</td>\n",
       "      <td>scott-surovell</td>\n",
       "      <td>State delegate</td>\n",
       "      <td>Virginia</td>\n",
       "      <td>democrat</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a floor speech.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>324.json</td>\n",
       "      <td>mostly-true</td>\n",
       "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
       "      <td>foreign-policy</td>\n",
       "      <td>barack-obama</td>\n",
       "      <td>President</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>democrat</td>\n",
       "      <td>70.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>Denver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id  truth-value                                               text  \\\n",
       "0   2635.json        false  Says the Annies List political group supports ...   \n",
       "1  10540.json    half-true  When did the decline of coal start? It started...   \n",
       "2    324.json  mostly-true  Hillary Clinton agrees with John McCain \"by vo...   \n",
       "\n",
       "                                topic            name                   job  \\\n",
       "0                            abortion    dwayne-bohac  State representative   \n",
       "1  energy,history,job-accomplishments  scott-surovell        State delegate   \n",
       "2                      foreign-policy    barack-obama             President   \n",
       "\n",
       "      state    politics  count1  count2  count3  count4  count5  \\\n",
       "0     Texas  republican     0.0     1.0     0.0     0.0     0.0   \n",
       "1  Virginia    democrat     0.0     0.0     1.0     1.0     0.0   \n",
       "2  Illinois    democrat    70.0    71.0   160.0   163.0     9.0   \n",
       "\n",
       "           context  \n",
       "0         a mailer  \n",
       "1  a floor speech.  \n",
       "2           Denver  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df of all the data\n",
    "df_liar = pd.read_csv(\"train.tsv\", encoding=\"utf8\", sep=\"\\t\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\"])\n",
    "\n",
    "df_liar.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classification formula to take out only statements labeled with 'true' or 'false'\n",
    "def classify(text):\n",
    "    if text == \"false\":\n",
    "        return 0\n",
    "    elif text == \"true\":\n",
    "        return 1 \n",
    "    else: \n",
    "        return -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add the class of truth-values\n",
    "df_liar[\"class\"] = df_liar[\"truth-value\"].apply(classify) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3671, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1123.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Health care reform legislation is likely to ma...</td>\n",
       "      <td>health-care</td>\n",
       "      <td>blog-posting</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>none</td>\n",
       "      <td>7.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>a news release</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12465.json</td>\n",
       "      <td>true</td>\n",
       "      <td>The Chicago Bears have had more starting quart...</td>\n",
       "      <td>education</td>\n",
       "      <td>robin-vos</td>\n",
       "      <td>Wisconsin Assembly speaker</td>\n",
       "      <td>Wisconsin</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a an online opinion-piece</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id truth-value                                               text  \\\n",
       "0   2635.json       false  Says the Annies List political group supports ...   \n",
       "3   1123.json       false  Health care reform legislation is likely to ma...   \n",
       "5  12465.json        true  The Chicago Bears have had more starting quart...   \n",
       "\n",
       "         topic          name                         job      state  \\\n",
       "0     abortion  dwayne-bohac        State representative      Texas   \n",
       "3  health-care  blog-posting                         NaN        NaN   \n",
       "5    education     robin-vos  Wisconsin Assembly speaker  Wisconsin   \n",
       "\n",
       "     politics  count1  count2  count3  count4  count5  \\\n",
       "0  republican     0.0     1.0     0.0     0.0     0.0   \n",
       "3        none     7.0    19.0     3.0     5.0    44.0   \n",
       "5  republican     0.0     3.0     2.0     5.0     1.0   \n",
       "\n",
       "                     context  class  \n",
       "0                   a mailer      0  \n",
       "3             a news release      0  \n",
       "5  a an online opinion-piece      1  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce the dataset to only true/false sentences\n",
    "df_reduced = df_liar[df_liar[\"class\"] != -1]\n",
    "print(df_reduced.shape)\n",
    "df_reduced.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using CountVectorizer\n",
    "First we will try the Logistic Regression using CountVectorizer, which converts the text to a matrix in which each token (here: each word of the full vocabulary of our statements) is represented with counts that tell how often the token occurs in each statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 7451)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "\n",
    "# training set\n",
    "X_train = count_vect.fit(df_reduced.text)\n",
    "X_train = count_vect.transform(df_reduced.text)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# subtract the classes\n",
    "y_train = df_reduced[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457, 7451)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create df of the test data\n",
    "df_liar_test = pd.read_csv(\"test.tsv\", encoding=\"utf8\", sep=\"\\t\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\"])\n",
    "df_liar_test[\"class\"] = df_liar_test[\"truth-value\"].apply(classify)\n",
    "df_test_reduced = df_liar_test[df_liar_test[\"class\"] != -1]\n",
    "\n",
    "# transform the test data to the right format, aligning with the training data \n",
    "# (so that it has the size of the vocab of the training set)\n",
    "X_test = count_vect.transform(df_test_reduced.text) \n",
    "y_test = df_test_reduced[\"class\"].values\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599562363239\n",
      "274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# we could try to evaluate the model\n",
    "logreg.fit(X_train, y_train)\n",
    "y_hat_test = logreg.predict(X_test)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "> This is not super high accuracy, but it is better than the multinomal regression we experimented with in update 1 (which was around 0.3). However, we don't really expect a high accuracy, since our dataset is not very dense, so this is already way better than we expected! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance\n",
    "We created a dictionary of all the features and their weighted importance, to check which features are the most important in the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.17203638 -0.27699619  0.05011177 ..., -0.15039476  0.27433863\n",
      "   0.47206197]]\n",
      "(1, 7451)\n"
     ]
    }
   ],
   "source": [
    "# this prints out all the coefficients (which is in fact all the vocabulary of the training set)\n",
    "print(logreg.coef_)\n",
    "print(logreg.coef_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of all coefficients and their weights from the logistic regression\n",
    "coef_dict = dict()\n",
    "for n, key in enumerate(count_vect.vocabulary_.keys()):\n",
    "    coef_dict[key] = logreg.coef_[0][n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "says : -0.172036379023\n",
      "the : -0.27699619287\n",
      "annies : 0.0501117682387\n",
      "list : 0.149425389135\n",
      "political : 0.117164536027\n",
      "group : -0.092538451089\n",
      "supports : 0.00853503997909\n",
      "third : 0.134349522517\n"
     ]
    }
   ],
   "source": [
    "# print some scores\n",
    "for x in list(coef_dict)[0:8]:\n",
    "    print (x, \":\", coef_dict[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ordering\n",
    "ordered_coefs = [(k, coef_dict[k]) for k in sorted(coef_dict, key=coef_dict.get, reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('opposes', 1.2852508564154546),\n",
       " ('scanners', 1.2302072443718222),\n",
       " ('duel', 1.223908577820521),\n",
       " ('democrats', 1.2135750192948989),\n",
       " ('dramatically', 1.1738511525095916),\n",
       " ('except', 1.1639518946957186),\n",
       " ('protects', 1.1618225704045153),\n",
       " ('predecessor', 1.1582500993003202),\n",
       " ('176', 1.1460457212623074),\n",
       " ('fiduciary', 1.133202658098218)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 highest weighted coefficients:\n",
    "ordered_coefs[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('answer', -1.0141049421385862),\n",
       " ('arm', -1.0344518907635407),\n",
       " ('sunset', -1.0349690821953719),\n",
       " ('1994', -1.0363564779213954),\n",
       " ('steroids', -1.0501622342165082),\n",
       " ('specter', -1.0730122253388568),\n",
       " ('canal', -1.1522027240337045),\n",
       " ('alligator', -1.2245222947742149),\n",
       " ('stopping', -1.2934142876362542)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 lowest weighted coefficients:\n",
    "ordered_coefs[-10:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "> As a result, the highest coefficients are the most important in determining a true statement, whereas the lowest coefficients are the most important in determining a false statement. \n",
    "\n",
    "\n",
    "> However, our training data is super sparse, and therefore these words might not be the best indicators of fake/true news statements. For example, the word 'scanners' is a highly neutral word, and I doubt whether it would be used relatively often in fake news. Same thing for the number '176'..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now perform dimensionality reduction, to check whether our model performs better when it's inputted a smaller matrix. We will compare the different reduced models by their accuracy. For the dimensionality reduction, we used the Singular Value Decomposition method described in Lab 8 (Vector Semantics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for them to fit in the SVD model, they will have to be converted to numpy matrices first\n",
    "# converting X_train to a matrix\n",
    "new_train = np.empty([3671, 7451])\n",
    "array_X_train = X_train.toarray()\n",
    "\n",
    "for n in range(3671):\n",
    "    new_train[n] = array_X_train[n]\n",
    "\n",
    "# converting X_test to a matrix\n",
    "new_test = np.empty([457, 7451])\n",
    "array_X_test = X_test.toarray()\n",
    "\n",
    "for n in range(457):\n",
    "    new_test[n] = array_X_test[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3671, 7451) (457, 7451)\n"
     ]
    }
   ],
   "source": [
    "print(new_train.shape, new_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# picking the different dimensions\n",
    "dimensions1 = 50\n",
    "dimensions2 = 100\n",
    "dimensions3 = 300\n",
    "dimensions4 = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 50 dimensions SVD\n",
    "train_SVD50Mat = decomposition.TruncatedSVD(n_components = dimensions1, algorithm = \"arpack\").fit_transform(new_train)\n",
    "test_SVD50Mat = decomposition.TruncatedSVD(n_components = dimensions1, algorithm = \"arpack\").fit_transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 50)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_SVD50Mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 dimensions SVD\n",
    "train_SVD100Mat = decomposition.TruncatedSVD(n_components = dimensions2, algorithm =\"arpack\").fit_transform(new_train)\n",
    "test_SVD100Mat = decomposition.TruncatedSVD(n_components = dimensions2, algorithm = \"arpack\").fit_transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 300 dimensions SVD\n",
    "train_SVD300Mat = decomposition.TruncatedSVD(n_components = dimensions3, algorithm = \"arpack\").fit_transform(new_train)\n",
    "test_SVD300Mat = decomposition.TruncatedSVD(n_components = dimensions3, algorithm = \"arpack\").fit_transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 400 dimensions SVD\n",
    "train_SVD400Mat = decomposition.TruncatedSVD(n_components = dimensions4, algorithm =\"arpack\").fit_transform(new_train)\n",
    "test_SVD400Mat = decomposition.TruncatedSVD(n_components = dimensions4, algorithm = \"arpack\").fit_transform(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492341356674\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "# evaluating 50 dimensions SVD model\n",
    "logreg.fit(train_SVD50Mat, y_train)\n",
    "y_hat_test_SVD = logreg.predict(test_SVD50Mat)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test_SVD))\n",
    "print(accuracy_score(y_test, y_hat_test_SVD, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.474835886214\n",
      "217\n"
     ]
    }
   ],
   "source": [
    "# evaluating 100 dimensions SVD model\n",
    "logreg.fit(train_SVD100Mat, y_train)\n",
    "y_hat_test_SVD100 = logreg.predict(test_SVD100Mat)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test_SVD100))\n",
    "print(accuracy_score(y_test, y_hat_test_SVD100, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.487964989059\n",
      "223\n"
     ]
    }
   ],
   "source": [
    "# evaluating 300 dimensions SVD model\n",
    "logreg.fit(train_SVD300Mat, y_train)\n",
    "y_hat_test_SVD300 = logreg.predict(test_SVD300Mat)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test_SVD300))\n",
    "print(accuracy_score(y_test, y_hat_test_SVD300, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.492341356674\n",
      "225\n"
     ]
    }
   ],
   "source": [
    "# evaluating 400 dimensions SVD model\n",
    "logreg.fit(train_SVD400Mat, y_train)\n",
    "y_hat_test_SVD400 = logreg.predict(test_SVD400Mat)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test_SVD400))\n",
    "print(accuracy_score(y_test, y_hat_test_SVD400, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Model | Accuracy \n",
    "--- | --- \n",
    "nonSVD | 0.59956\n",
    "SVD50 | 0.49234\n",
    "SVD100 | 0.47483\n",
    "SVD300 | 0.48796\n",
    "SVD400 | 0.49234\n",
    "\n",
    "> As you can see in the table above, the higher the dimension, the higher the accuracy. The model that works the best is the one in which the dimensions have not been reduced. We did not expect this, because since we're using a sparse vector, we thought the reduction might help out to make the regression easier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using TfidfVectorizer\n",
    "Instead of the `CountVectorizer`, we now tried the Logistic Regression using the `TfidfVectorizer`. This vectorizer converts the statements to a matrix of TF-IDF features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3671, 7451)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# creating the training vector\n",
    "X_train_tfidf = tfidf_vect.fit(df_reduced.text)\n",
    "X_train_tfidf = tfidf_vect.transform(df_reduced.text)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fitting the training data\n",
    "logreg.fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(457, 7451)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating the test vector\n",
    "X_test_tfidf = tfidf_vect.transform(df_test_reduced.text) \n",
    "X_test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.628008752735\n",
      "287\n"
     ]
    }
   ],
   "source": [
    "# evaluating the tfidf model\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_hat_test = logreg.predict(X_test_tfidf)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "> This gives about 0.03 higher accuracy than the regression using `CountVectorizer`. This is not a super significant difference, but it at least suggests that we could probably better usse the `TfifVectorizer` for our multinomial regression too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "Again, we'll perform Singular Value Decomposition to see whether the reduction of dimensions will change the outcome of the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting train to a matrix\n",
    "new_train_tfidf = np.empty([3671, 7451])\n",
    "array_X_train_tfidf = X_train_tfidf.toarray()\n",
    "\n",
    "for n in range(3671):\n",
    "    new_train_tfidf[n] = array_X_train_tfidf[n]\n",
    "\n",
    "# converting test to a matrix\n",
    "new_test_tfidf = np.empty([457, 7451])\n",
    "array_X_test_tfidf = X_test_tfidf.toarray()\n",
    "\n",
    "for n in range(457):\n",
    "    new_test_tfidf[n] = array_X_test_tfidf[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dimensions\n",
    "dimensions1 = 50\n",
    "dimensions2 = 100\n",
    "dimensions3 = 300\n",
    "dimensions4 = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 50 dimensions\n",
    "train_SVD50Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions1, algorithm = \"arpack\").fit_transform(new_train_tfidf)\n",
    "test_SVD50Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions1, algorithm = \"arpack\").fit_transform(new_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 100 dimensions\n",
    "train_SVD100Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions2, algorithm = \"arpack\").fit_transform(new_train_tfidf)\n",
    "test_SVD100Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions2, algorithm = \"arpack\").fit_transform(new_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 300 dimensions \n",
    "train_SVD300Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions3, algorithm = \"arpack\").fit_transform(new_train_tfidf)\n",
    "test_SVD300Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions3, algorithm = \"arpack\").fit_transform(new_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 400 dimensions \n",
    "train_SVD400Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions4, algorithm = \"arpack\").fit_transform(new_train_tfidf)\n",
    "test_SVD400Mat_tfidf = decomposition.TruncatedSVD(n_components = dimensions4, algorithm = \"arpack\").fit_transform(new_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544857768053\n",
      "249\n"
     ]
    }
   ],
   "source": [
    "# evaluating 50 dimensions tfidf model\n",
    "logreg.fit(train_SVD50Mat_tfidf, y_train)\n",
    "y_hat_test = logreg.predict(test_SVD50Mat_tfidf)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.584245076586\n",
      "267\n"
     ]
    }
   ],
   "source": [
    "# evaluating 100 dimensions tfidf model\n",
    "logreg.fit(train_SVD100Mat_tfidf, y_train)\n",
    "y_hat_test = logreg.predict(test_SVD100Mat_tfidf)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599562363239\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "# evaluating 300 dimensions tfidf model\n",
    "logreg.fit(train_SVD300Mat_tfidf, y_train)\n",
    "y_hat_test = logreg.predict(test_SVD300Mat_tfidf)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.599562363239\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "# evaluating 400 dimensions tfidf model\n",
    "logreg.fit(train_SVD400Mat_tfidf, y_train)\n",
    "y_hat_test = logreg.predict(test_SVD400Mat_tfidf)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "Model | Accuracy \n",
    "--- | --- \n",
    "nonSVD | 0.62801\n",
    "SVD50 | 0.54486\n",
    "SVD100 | 0.58425\n",
    "SVD300 | 0.59956\n",
    "SVD400 | 0.59956\n",
    "> As you can see in the table above, there is again better accuracy when the dimensions get higher. Furthermore, all of the different models perform better than their significances of the `CountVectorizer` models. Also, the SVD300 and SVD400 perform exactly the same as the best `CountVectorizer` model. So, our advise for the multinomial regression would be to use the nonSVD TF-IDF model! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Filtering out stopwords\n",
    "Here we check whether filtering out stopwords, digits and applying lemmatization will better the accuracy of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocessing function to lemmatize and to filter out unimportant words. \n",
    "def filtering(text):\n",
    "    lostrings = text.split(' ')\n",
    "    new_lostrings = []\n",
    "    for word in lostrings:\n",
    "        word = nltk.WordNetLemmatizer().lemmatize(\n",
    "                word.translate(str.maketrans('', '', string.punctuation)).lower()) # remove punctuation & lemmatize\n",
    "        if word not in stopwords.words('english') and not word.isdigit(): # remove stopwords & digits\n",
    "            new_lostrings.append(word)\n",
    "    return ' '.join(new_lostrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello nice warm chill day today let go take bike ride'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check if it works\n",
    "filtering(\"Hello, it is a nice and warm and chill day today, let's go and take our 2 bikes out for a ride!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# applying it to the statements of the training set\n",
    "df_reduced[\"filteredtext\"] = df_reduced[\"text\"].apply(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/Anaconda/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# applying it to the statements of the test set\n",
    "df_test_reduced[\"filteredtext\"] = df_test_reduced[\"text\"].apply(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "      <th>class</th>\n",
       "      <th>filteredtext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0</td>\n",
       "      <td>say annies list political group support thirdt...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id truth-value                                               text  \\\n",
       "0  2635.json       false  Says the Annies List political group supports ...   \n",
       "\n",
       "      topic          name                   job  state    politics  count1  \\\n",
       "0  abortion  dwayne-bohac  State representative  Texas  republican     0.0   \n",
       "\n",
       "   count2  count3  count4  count5   context  class  \\\n",
       "0     1.0     0.0     0.0     0.0  a mailer      0   \n",
       "\n",
       "                                        filteredtext  \n",
       "0  say annies list political group support thirdt...  "
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the df to see if it works\n",
    "df_reduced.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer()\n",
    "\n",
    "# creating the training vector\n",
    "X_train_filter = tfidf_vect.fit(df_reduced.filteredtext)\n",
    "X_train_filter = tfidf_vect.transform(df_reduced.filteredtext)\n",
    "\n",
    "# creating the test vector\n",
    "X_test_filter = tfidf_vect.transform(df_test_reduced.filteredtext) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.597374179431\n",
      "273\n"
     ]
    }
   ],
   "source": [
    "# evaluating the filtered tfidf model\n",
    "logreg.fit(X_train_filter, y_train)\n",
    "y_hat_test = logreg.predict(X_test_filter)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "> As you can see from the accuracy, the model doesn't really work any better with more preprocessing, sadly enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. New features: sentiment & poltical preference\n",
    "Here, we will check whether the model works better if we were to give each sentence a sentiment score, and a political score. More on this progress is described in the `Multinomial-LR-TFIDF` notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_liar_sentiment = pd.read_csv(\"sentiment_train.csv\", encoding=\"utf8\", sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll only focus on the 3 most occuring political preferences\n",
    "politics = dict({\"republican\": 0, \"democrat\" : 1, \"none\" : 2})\n",
    "\n",
    "# classification formula for political background\n",
    "def classify_politics(text):\n",
    "    if text not in politics.keys(): \n",
    "        return -1\n",
    "    else:\n",
    "        return politics[text]\n",
    "\n",
    "# add this new class of politic scores\n",
    "df_liar_sentiment[\"political-score\"] = df_liar_sentiment[\"politics\"].apply(classify_politics)\n",
    "# also add truth scores\n",
    "df_liar_sentiment[\"truth-score\"] = df_liar_sentiment[\"truth-value\"].apply(classify) \n",
    "# remove the ones we don't want\n",
    "df_liar_sentiment = df_liar_sentiment[df_liar_sentiment[\"political-score\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>truth-value</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>name</th>\n",
       "      <th>job</th>\n",
       "      <th>state</th>\n",
       "      <th>politics</th>\n",
       "      <th>count1</th>\n",
       "      <th>count2</th>\n",
       "      <th>count3</th>\n",
       "      <th>count4</th>\n",
       "      <th>count5</th>\n",
       "      <th>context</th>\n",
       "      <th>pos-sentiment</th>\n",
       "      <th>neg-sentiment</th>\n",
       "      <th>political-score</th>\n",
       "      <th>truth-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2635.json</td>\n",
       "      <td>false</td>\n",
       "      <td>Says the Annies List political group supports ...</td>\n",
       "      <td>abortion</td>\n",
       "      <td>dwayne-bohac</td>\n",
       "      <td>State representative</td>\n",
       "      <td>Texas</td>\n",
       "      <td>republican</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>a mailer</td>\n",
       "      <td>0.007972</td>\n",
       "      <td>0.012908</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id truth-value                                               text  \\\n",
       "0  2635.json       false  Says the Annies List political group supports ...   \n",
       "\n",
       "      topic          name                   job  state    politics  count1  \\\n",
       "0  abortion  dwayne-bohac  State representative  Texas  republican     0.0   \n",
       "\n",
       "   count2  count3  count4  count5   context  pos-sentiment  neg-sentiment  \\\n",
       "0     1.0     0.0     0.0     0.0  a mailer       0.007972       0.012908   \n",
       "\n",
       "   political-score  truth-score  \n",
       "0                0            0  "
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_liar_sentiment.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test_sent = pd.read_csv(\"sentiment_test.csv\", encoding=\"utf8\", sep=\",\", names=[\"id\", \"truth-value\", \n",
    "                                                                     \"text\", \"topic\", \"name\", \"job\", \n",
    "                                                                     \"state\", \"politics\", \"count1\", \"count2\", \n",
    "                                                                     \"count3\", \"count4\", \"count5\", \"context\", \n",
    "                                                                                        \"pos-sentiment\", \"neg-sentiment\"])\n",
    "df_test_sent[\"truth-score\"] = df_test_sent[\"truth-value\"].apply(classify)\n",
    "df_test_sent[\"political-score\"] = df_test_sent[\"politics\"].apply(classify_politics)\n",
    "\n",
    "df_test_sent = df_test_sent[df_test_sent[\"political-score\"] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3671, 7451) (457, 7451)\n"
     ]
    }
   ],
   "source": [
    "# the shapes of the matrices created for SVD\n",
    "print(new_train_tfidf.shape, new_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.00797194  0.01290816  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.01148072  0.0142225   1.        ]\n",
      " [ 0.          0.          0.         ...,  0.00925808  0.01196854  1.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.01047535  0.01883803  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.00801672  0.01683131  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.01071429  0.01392857  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we need to add 3 new columns to the matrices, which we already created for the SVD \n",
    "# (new_train_tfidf & new_test_tfidf) : one column for the positive sentiment score, \n",
    "# one column for the negative score and one column for the political score. \n",
    "X_train_sent = np.empty([3671, 7454])\n",
    "\n",
    "for n in range(3671):\n",
    "    pos = df_liar_sentiment[\"pos-sentiment\"].values[n]\n",
    "    neg = df_liar_sentiment[\"neg-sentiment\"].values[n]\n",
    "    pol = df_liar_sentiment[\"political-score\"].values[n]\n",
    "    X_train_sent[n] = np.append(new_train_tfidf[n], [pos, neg, pol])\n",
    "\n",
    "print(X_train_sent)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...,  0.00797194  0.01290816  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.01148072  0.0142225   1.        ]\n",
      " [ 0.          0.          0.         ...,  0.00925808  0.01196854  1.        ]\n",
      " ..., \n",
      " [ 0.          0.          0.         ...,  0.01338156  0.01315552  0.        ]\n",
      " [ 0.          0.          0.         ...,  0.01366709  0.01313776  1.        ]\n",
      " [ 0.          0.          0.         ...,  0.01154128  0.01324649  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# we also need to do this for the test set.\n",
    "X_test_sent = np.empty([457, 7454])\n",
    "\n",
    "for n in range(457):\n",
    "    pos = df_liar_sentiment[\"pos-sentiment\"].values[n]\n",
    "    neg = df_liar_sentiment[\"neg-sentiment\"].values[n]\n",
    "    pol = df_liar_sentiment[\"political-score\"].values[n]\n",
    "    X_test_sent[n] = np.append(new_test_tfidf[n], [pos, neg, pol])\n",
    "\n",
    "print(X_test_sent) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.63238512035\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# evaluating the new model\n",
    "logreg.fit(X_train_sent, y_train)\n",
    "y_hat_test = logreg.predict(X_test_sent)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test, y_hat_test))\n",
    "print(accuracy_score(y_test, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "> Woohoo, finally something that *does* work at least a bit better. So adding sentiment and political preference makes a tiny little improvement on the binomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing with a new dataset\n",
    "Here, we tested our best binomial model, namely the tfidf regression model (since this data doesn't have political information, we cannot use the tfidf + political features model above), on a different dataset with labeled fake/true news headlines that we found at [Kaggle](https://www.kaggle.com/jruvika/fake-news-detection). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# opening the new csv file\n",
    "df_more = pd.read_csv(\"more-fake-news.csv\", encoding=\"utf8\", sep=\",\")\n",
    "df_more.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4009, 7451)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_new= tfidf_vect.transform(df_more.Headline) \n",
    "y_test_new = df_more[\"Label\"].values\n",
    "X_test_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.516587677725\n",
      "2071\n"
     ]
    }
   ],
   "source": [
    "# evaluating the tfidf model\n",
    "logreg.fit(X_train_tfidf, y_train)\n",
    "y_hat_test = logreg.predict(X_test_new)\n",
    "\n",
    "# evaluate using accuracy: proportion of correctly predicted over total\n",
    "print(accuracy_score(y_test_new, y_hat_test))\n",
    "print(accuracy_score(y_test_new, y_hat_test, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments\n",
    "> As you can see, it doesn't really perform well on other test sets, and therefore our model is probably not so scalable. This is because it is trained on short statements, so it probably needs way more text if we want it to perform better..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
